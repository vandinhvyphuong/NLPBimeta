{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "import glob, os, time, sys\n",
                "import json\n",
                "import numpy as np\n",
                "from collections import defaultdict\n",
                "import argparse\n",
                "\n",
                "from dataset.genome import GenomeDataset\n",
                "from dataset.utils import load_meta_reads, create_document\n",
                "import utils.utils as utils\n",
                "from debug.visualize import get_group_label, visualize\n",
                "from utils.metrics import genome_acc, group_precision_recall\n",
                "\n",
                "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "from multiprocessing import Pool, cpu_count\n",
                "\n",
                "from sklearn.cluster import KMeans"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "cores = cpu_count()\n",
                "cores"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "8"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 50
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "sys.path.append('.')\n",
                "\n",
                "DATASET_DIR = '../../Data/reads/'                   # Raw fasta data dir\n",
                "BIMETAOUT_DIR = '../../Data/bimetaout/20210904/'    # bimeta output dir\n",
                "DATASET_NAME = 'S1'                                 # Specifc fasta dataset or all of them\n",
                "RESULT_DIR = '../../Data/doc2vecbimetaout/'    # Result dir"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "source": [
                "# Hyperparameters\n",
                "KMERS = [4]\n",
                "VECTOR_SIZE = 10\n",
                "WINDOW_SIZE = 8\n",
                "EPOCHS = 20\n",
                "WORKERS = 2"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "# Mapping of dataset and its corresponding number of clusters\n",
                "with open('config/dataset_metadata.json', 'r') as f:\n",
                "    n_clusters_mapping = json.load(f)['datasets']"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "# Get some parameters\n",
                "dataset_file = os.path.join(DATASET_DIR, DATASET_NAME + '.fna')\n",
                "dataset_name = os.path.basename(dataset_file).split('.fna')[0]\n",
                "\n",
                "print(\"-------------------------------------------------------\")\n",
                "print('Processing dataset: ', dataset_name)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "-------------------------------------------------------\n",
                        "Processing dataset:  S1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "n_clusters = n_clusters_mapping[dataset_name]\n",
                "print('Prior number of clusters: ', n_clusters)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Prior number of clusters:  2\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "t0 = time.time()\n",
                "# Load group file (phase 1 of bimeta) according to dataset_name\n",
                "groups, seeds = utils.load_groups_seeds(BIMETAOUT_DIR, dataset_name)\n",
                "print('Total number of groups: ', len(groups))\n",
                "print('Time to load groups: ', (time.time() - t0))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of groups:  152\n",
                        "Time to load groups:  0.01984119415283203\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "# Read fasta dataset\n",
                "t1 = time.time()\n",
                "reads, labels = load_meta_reads(dataset_file, type='fasta')\n",
                "print('Total number of reads: ', len(labels))\n",
                "print('Time to load reads: ', (time.time() - t1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of reads:  96367\n",
                        "Time to load reads:  5.307618618011475\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "t2 = time.time()\n",
                "# Creating document from reads...\n",
                "dict, docs = create_document(reads, KMERS)\n",
                "\n",
                "# Tokenization of each document\n",
                "#tokenized_docs = []\n",
                "#for doc in docs:\n",
                "#    tokenized_docs.append(word_tokenize(doc.lower()))\n",
                "\n",
                "# Convert tokenized document into gensim formated tagged data\n",
                "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(docs)]\n",
                "\n",
                "print('Time to create docs from reads: ', (time.time() - t2))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Time to create docs from reads:  3.607961893081665\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "t3 = time.time()\n",
                "# Train doc2vec model\n",
                "model = Doc2Vec(tagged_data, \n",
                "                vector_size = VECTOR_SIZE, \n",
                "                window = WINDOW_SIZE, \n",
                "                workers = WORKERS, \n",
                "                epochs = EPOCHS)\n",
                "\n",
                "# Save trained doc2vec model\n",
                "model_file = os.path.join(RESULT_DIR, dataset_name + '.doc2vec.model')\n",
                "model.save(model_file)\n",
                "\n",
                "# Load saved doc2vec model\n",
                "#model= Doc2Vec.load(model_file)\n",
                "\n",
                "print('Doc2Vec model training time: ', (time.time() - t3))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Doc2Vec model training time:  37.13066864013672\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "source": [
                "t4 = time.time()\n",
                "print('Compute doc2vec feature ...')\n",
                "ndocs = len(docs)\n",
                "doc2vec = np.zeros((ndocs, VECTOR_SIZE))\n",
                "for i in range(ndocs):\n",
                "    doc2vec[i] = model.infer_vector(docs[i])\n",
                "\n",
                "print('Compute doc2vec feature time: ', (time.time() - t4))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Compute doc2vec feature ...\n",
                        "Compute doc2vec feature time:  72.98422932624817\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "source": [
                "t5 = time.time()\n",
                "print('Compute group feature ...')\n",
                "ngroups = len(seeds)\n",
                "doc2vec_group_features = np.zeros((ngroups, VECTOR_SIZE))\n",
                "for i in range(ngroups):\n",
                "    doc2vec_group_features[i] = np.mean([doc2vec[idx] for idx in seeds[i]])\n",
                "print('Compute group feature time: ', (time.time() - t5))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Compute group feature ...\n",
                        "Compute group feature time:  0.03279876708984375\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "source": [
                "# Clustering groups\n",
                "t6 = time.time()\n",
                "print('Clustering ...')\n",
                "kmeans = KMeans(\n",
                "    init=\"random\",\n",
                "    n_clusters=n_clusters,\n",
                "    n_init=100,\n",
                "    max_iter=200,\n",
                "    random_state=20210905)\n",
                "kmeans.fit(X=doc2vec_group_features, y=labels)\n",
                "y_pred_kmeans = kmeans.predict(X=doc2vec_group_features)\n",
                "print('Clustering time: ', (time.time() - t6))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Clustering ...\n",
                        "Clustering time:  0.1766681671142578\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "source": [
                "# Map read to group and compute F-measure\n",
                "t6 = time.time()\n",
                "groupPrec = group_precision_recall(labels, groups, n_clusters)[0]\n",
                "f1 = genome_acc(groups, y_pred_kmeans, labels, n_clusters)[2]\n",
                "print('Compute measures: ')\n",
                "print('Group precision: ', groupPrec)\n",
                "print('F1-score: ', f1)\n",
                "print('Total time: ', (time.time() - t0))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Compute measures: \n",
                        "Group precision:  0.9898824286322081\n",
                        "F1-score:  0.7120773447036292\n",
                        "Total time:  2809.8840293884277\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.13 64-bit ('bimeta': conda)"
        },
        "interpreter": {
            "hash": "5cef0a9f8ded764309eccbc26778b077c5ace5f1e81d93105e2fc9ddc071b567"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}